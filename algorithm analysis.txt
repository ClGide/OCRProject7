After stating the 0/1 Knapsack problem we will compare the time and space complexity 
of the brute-force and dynamic programming solution. When speaking of space complexity we mean 
additional space complexity => the variables the function needs aside from the input. 
Finally, we will discuss the limitations of the optimized solution.


We are given a finite number of items. Each item has a value and a weight. We want to maximize the 
value of the set of items we will choose. However, there is a constraint. The set of items we choose 
cannot surpass a given weight. 
In our case, the items are the shares. The weights are their prices and the values are the 
returns we will get after two years. The values are expressed as a percentage of the weight.   


The idea behind the brute-force algorithm is straigth-forward. First, it needs to compute the expected 
profit for each share. The time complexity is O(N) : we loop through all the items doing one computation
for each. The space complexity is O(N/2) : the function takes two lists (the 
percentage from the shares yielded after two years and the shares' price) and we need a third one of equal 
length to store the results. 
  
Second, it computes all the possible combinations of items. If, e.g, there are six items, it will compute 
all the combinations of one item, two items, three items and so on up to six. The time complexity of a nested
for loop is O(N**2). Although the use of a generator is efficient, it's space complexity is O(N). 

Third, we need to compute the value and weight of each combination. 
For each combination we need to do two computations inside a for loop. The time complexity is therefore O(N**2).
We use two variables to loop through the combinations but as the input grows, it becomes irrelevant.   

Fourth, we use the built-in max on the list of tuples containing the value and weight of each combination. 
The time complexity is O(N) and the operation requires no additional space. 

If we do the sum, it seems the time complexity is O(N) + O(N**2) + O(N**2) = O(2N**2 + N). As the size of the input grows,
that's the same as N**2 (quadratic time).
space complexity is O(N/2) + O(N). That's a linear growth. 


The general idea behind any dynamic programming(DP) solution is "simplifying a complicated problem by breaking it down into simpler 
sub-problem" (wiki). We then use the solution to those subproblems to solve the problem. 
In our case, given a max capacity (max_c), we want to know the maximum value we can get out of a set of n items. The DP algorithm breaks
down the set into n subsets. The first subset contains the first item, the second subset contains the first two items and so on up
to the full set. Thus, between subset n and subset n-1, the only difference is that subset n contains item n. 
For each of the subsets, the algorithm answers max_c questions. In other words, if max capacity was 10, 
10 questions are answered for each subset : what's the max value we can extract out of the subset given capacity (c) 1 ? 
what's the max value we can extract out of the subset given capacity 2 ? what's the max value we can extract out of the subset
given capacity 3 ? and so on up to max_c.  
Although this might seem more time and space consuming then the brute-force solution, it's not. By leveraging the below two info, 
the DP algorithm drastically reduces the number of computations needed : 
- if the weight of the subset n is greater than c, we can retrieve the answer from a previous question. Why ? Because if the subset n 
is heavier than c, it means the item n cannot be taken. Thus, the maximum value we can get out of the subset n is the same as the 
maximum value we can get out of the subset n-1.  
- if the subset n is as heavy or lighter than c, than we have two options. We either take item n or we do not take it. The first 
option is the maximum value we can get out of the subset n-1 given capacity c. The second is the sum of the value of item n and the 
maximum value we can extract out c - weight of item n. 
The last subproblem solved by the algorithm is also the solution to the problem. Given the last subset that's identical to the set and 
c that's equal to max_c, what's the maximum value we can extract ? 

At this point, we know the maximum value we can extract out of the set of items. However, we do not know the items we need to choose to attain
that value. In order to discover the items we need to take, we go through our previous answers in reverse order. 
Remember the two info we leveraged to efficiently discover the maximum value ? We are going to leverage them to discover the items we need to take. 
Indeed, the answer (1) to the question "what's the maximum value we can get out of the subset n given capacity c ?" is either identical or different 
to the answer (2) to the question "what's the maximum value we can get out of the subset n-1 given capacity c ?". If (1) and (2) are identical, it means 
the item n that differentiates the subset n from the subset n-1 wasn't taken. We can then ask the same questions with subsets n-1 and n-2.  
However, if (1) and (2) are different, it means the item n was indeed taken. Thus, when asking the quetion with subsets n-1 and n-2 we need to take the 
weight of the item into account. In other words, c isn't max_c anymore but c - weight of the taken item. 

For implementing the DP algorithm we create an empty table. There are c rows and n columns. We complete the table sequentially, one row after the other. 
Completing the case at row 3, column 4 amounts to answering the question "given capacity 3, what's the maximum value we can extract of the subset containing
the first 4 items of the set ?"

Now, let's analyze the algorithm's complexity. Before coming to the solution on it's know, we need to prepare the data. Five operations are needed. 
The first operation is reading the excel stylesheet containing the shares: time complexity O(N) and no additional variable is needed.
The second operation is going through the data and creating three lists: time complexity O(N) and three additional variables are needed O(3N). 
The third operation is creating a dict mapping the share's weight to it's name: time complexity O(N) and one additional variable is needed O(N).
The fourth operation is cleaning the three lists. We need to loop through each of them. time complexity O(N) and no additional variable is needed.
The fifth operation is computing the profits in absolute value from the percentages of the shares: time complexity O(N) and space complexity is O(N/2).
If we do the sum, time complexity is O(4N) which is linear time. The space complexity is O(4N + N/2) which is also linear.  


After those five operation, the data is ready and we can use the DP solution. The function takes three args. Two lists of equal length n (weights and values)
and an integer max_c (the capacity constraint). The time and space complexity of the DP do not depend solely on the size of the input. 
The capacity constraint (max_c) can be the predominant variable. However, in order to be able to calculate, we will suppose that max_c is in the same order of
magnitude as the number of items in the to lists. 
The first operation is building an empty table. We need a nested loop and a n number of lists. The time complexity is thus O(N**2) while the space complexity 
is O(N**2).
The second operation is going through the table. We use a nested loop but we do not need additional variables. The time complexity is thus O(N**2).
The third operation is going backward through the table to identify the needed items. We need a nested loop and a list holding a reference to chosen items. The 
time complexity is therefore O(N**2) while the space complexity is O(N).  
If we do the sum, time complexity is O(3N**2) which is quadratic time. The space complexity is O(N**2+N) which is also quadratic.

Before returning the value we do some formatting which involve going through the list. In the big picture, this won't change the result. Thus, if we sum up 
the complexity of what happens before the DP algo with what happens in it, we get the following. Time complexity is quadratic (as the input scales up, the linear
becomes irrelevant). The same goes for space complexity.      

 

One of the limitations of the dynamic programming solution is that it needs to convert the weights to integers. 
A second limitation is that the capacity constraint (max_c) cannot be too high, even if the number of items stays reasonable. The problem is that we need 
to create max_c columns. 
